{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":2739456,"sourceType":"datasetVersion","datasetId":1670098}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install peft\n!pip install bitsandbytes\n!pip install accelerate","metadata":{"execution":{"iopub.status.busy":"2024-07-19T16:35:11.164018Z","iopub.execute_input":"2024-07-19T16:35:11.164358Z","iopub.status.idle":"2024-07-19T16:35:55.997948Z","shell.execute_reply.started":"2024-07-19T16:35:11.164330Z","shell.execute_reply":"2024-07-19T16:35:55.996817Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting peft\n  Downloading peft-0.11.1-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.1)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.1.2)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.41.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.66.4)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.30.1)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.3)\nRequirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.23.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2024.3.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft) (3.1.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.12.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2023.12.25)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.19.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.2.2)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\nDownloading peft-0.11.1-py3-none-any.whl (251 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.6/251.6 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: peft\nSuccessfully installed peft-0.11.1\nCollecting bitsandbytes\n  Downloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl.metadata (2.2 kB)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (2.1.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (1.12.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (2024.3.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (2.1.3)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\nDownloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes\nSuccessfully installed bitsandbytes-0.43.1\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.30.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.1)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.1.2)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.23.2)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.1.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2024.3.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.66.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport gc\nimport torch\nimport peft\nimport torchaudio\nimport accelerate\nimport numpy as np\nimport pandas as pd\nfrom typing import Any\nimport bitsandbytes as bnb\nfrom dataclasses import dataclass\nfrom torch.utils.data import Dataset\nfrom peft import get_peft_model, LoraConfig, TaskType, prepare_model_for_kbit_training\nfrom transformers import WhisperForConditionalGeneration, Seq2SeqTrainingArguments, Seq2SeqTrainer, WhisperTokenizer, WhisperProcessor, DataCollatorForSeq2Seq, BitsAndBytesConfig\n\nprocessor = WhisperProcessor.from_pretrained(\"openai/whisper-large-v3\", language='en', task = \"transcribe\")\nwhisper_tokenizer = WhisperTokenizer.from_pretrained('openai/whisper-large-v3', language = 'en', task = \"transcribe\")\ntrain_dataset = torchaudio.datasets.LIBRISPEECH('/kaggle/input/librispeech-clean', url='train-clean-360', download=False)\n#bart_model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large\", forced_bos_token_id=0)\n#bart_tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large\", language='en')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-19T17:23:45.458398Z","iopub.execute_input":"2024-07-19T17:23:45.459044Z","iopub.status.idle":"2024-07-19T17:24:37.541288Z","shell.execute_reply.started":"2024-07-19T17:23:45.459015Z","shell.execute_reply":"2024-07-19T17:24:37.540276Z"},"trusted":true},"execution_count":64,"outputs":[{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"}]},{"cell_type":"code","source":"class training_dataset(Dataset) :\n    def __init__(self, dataset) :\n        super().__init__()\n        self.data = dataset\n    def __len__(self) :\n        return self.data.__len__()\n    \n    def __getitem__(self, idx) :\n        data = processor(self.data[idx][0].numpy(), sampling_rate = 16000, truncation=True, padding_size=3000, return_tensors='pt', return_attention_mask=True)\n        data['labels'] = whisper_tokenizer(self.data[idx][2], padding='longest', truncation=True, max_length=100, return_tensors='pt').input_ids\n        return data\n    \ndataset = training_dataset(train_dataset)","metadata":{"execution":{"iopub.status.busy":"2024-07-19T17:24:37.543253Z","iopub.execute_input":"2024-07-19T17:24:37.543915Z","iopub.status.idle":"2024-07-19T17:24:37.551184Z","shell.execute_reply.started":"2024-07-19T17:24:37.543878Z","shell.execute_reply":"2024-07-19T17:24:37.550258Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"class CustomWhisperModel(WhisperForConditionalGeneration):\n    def __init__(self, model_name) :\n        super().__init__(model_name)\n        \n    def forward(self, input_ids=None,\n                    input_features=None,\n                    inputs_embeds = None,\n                    attention_mask=None,\n                    decoder_input_ids=None,\n                    decoder_attention_mask=None,\n                    labels=None,\n                    decoder_inputs_embeds = None,\n                    output_attentions=None,\n                    output_hidden_states=None,\n                    return_dict=None,\n                    output_attention= None,\n                    task_type =None):\n        \n        inputs = {\"input_features\": input_ids, 'decoder_input_ids' : decoder_input_ids, 'attention_mask' : attention_mask, 'decoder_attention_mask' : decoder_attention_mask,\n                 'labels' : labels, 'return_dict' : return_dict, 'output_hidden_states' : output_hidden_states, 'output_attentions' : output_attention}\n        if input_features != None : \n            inputs['input_features'] = input_features    \n\n        outputs = super().forward(**inputs)\n        return outputs\n    \nbnb_config = BitsAndBytesConfig(load_in_8bit=True)\ncustom_model = CustomWhisperModel.from_pretrained(\"openai/whisper-large-v3\", quantization_config = bnb_config, device_map='auto')\nq_model = prepare_model_for_kbit_training(custom_model)\n\npeft_config = LoraConfig(task_type=TaskType.SEQ_2_SEQ_LM, inference_mode=False, target_modules=[\"q_proj\", \"v_proj\"], r=32, lora_alpha=64, lora_dropout=0.1)\nfinal_model = get_peft_model(q_model, peft_config)","metadata":{"execution":{"iopub.status.busy":"2024-07-19T17:16:01.066111Z","iopub.execute_input":"2024-07-19T17:16:01.066779Z","iopub.status.idle":"2024-07-19T17:16:06.292994Z","shell.execute_reply.started":"2024-07-19T17:16:01.066748Z","shell.execute_reply":"2024-07-19T17:16:06.292129Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"def apply_masking(text, mask_rate=0.07):\n    mask = torch.rand(text.shape) > mask_rate\n    text = text * mask\n    return text\n\n@dataclass\nclass DataCollatorSpeechSeq2SeqWithPadding:\n    processor: Any\n\n    def __call__(self, features):\n        input_features = [{\"input_features\": apply_masking(feature[\"input_features\"].squeeze(0))} for feature in features]\n        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n\n        label_features = [{\"input_ids\": feature[\"labels\"].squeeze(0)} for feature in features]\n        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n\n        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n\n        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n            labels = labels[:, 1:]\n\n        batch[\"labels\"] = labels\n        return batch\n    \ncollator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)","metadata":{"execution":{"iopub.status.busy":"2024-07-19T16:56:56.891418Z","iopub.execute_input":"2024-07-19T16:56:56.891766Z","iopub.status.idle":"2024-07-19T16:56:56.901518Z","shell.execute_reply.started":"2024-07-19T16:56:56.891741Z","shell.execute_reply":"2024-07-19T16:56:56.900625Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"def total_params(model):\n    return sum(p.numel() for p in model.parameters())\n\nprint(f'Memory used by model: {round(final_model.get_memory_footprint()/1024/1024/1024, 2)} GB')\nprint(f'total number of parameters is {total_params(final_model)}')\nfinal_model.print_trainable_parameters()","metadata":{"execution":{"iopub.status.busy":"2024-07-19T16:57:00.079583Z","iopub.execute_input":"2024-07-19T16:57:00.079913Z","iopub.status.idle":"2024-07-19T16:57:00.128596Z","shell.execute_reply.started":"2024-07-19T16:57:00.079888Z","shell.execute_reply":"2024-07-19T16:57:00.127595Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"Memory used by model: 1.71 GB\ntotal number of parameters is 1559219200\ntrainable params: 15,728,640 || all params: 1,559,219,200 || trainable%: 1.0088\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import TrainerState, TrainerCallback, TrainerControl\nfrom transformers.trainer_utils import PREFIX_CHECKPOINT_DIR\n\nclass SavePeftModelCallback(TrainerCallback):\n    def on_save(\n        self,\n        args: Seq2SeqTrainingArguments,\n        state: TrainerState,\n        control: TrainerControl,\n        **kwargs,\n    ):\n        checkpoint_folder = os.path.join(args.output_dir, f\"{PREFIX_CHECKPOINT_DIR}-{state.global_step}\")\n\n        peft_model_path = os.path.join(checkpoint_folder, \"adapter_model\")\n        kwargs[\"model\"].save_pretrained(peft_model_path)\n\n        pytorch_model_path = os.path.join(checkpoint_folder, \"pytorch_model.bin\")\n        if os.path.exists(pytorch_model_path):\n            os.remove(pytorch_model_path)\n        return control","metadata":{"execution":{"iopub.status.busy":"2024-07-19T17:14:37.196796Z","iopub.execute_input":"2024-07-19T17:14:37.197665Z","iopub.status.idle":"2024-07-19T17:14:37.204079Z","shell.execute_reply.started":"2024-07-19T17:14:37.197637Z","shell.execute_reply":"2024-07-19T17:14:37.203172Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"/kaggle/working/\",\n    report_to=\"none\",\n    per_device_train_batch_size=32,\n    gradient_accumulation_steps=1, \n    learning_rate=35e-6,\n    warmup_steps=50,\n    num_train_epochs=1,\n    max_steps=3000,\n    logging_steps=10,\n    fp16=True,\n    remove_unused_columns=False,\n    label_names=[\"labels\"],\n)\ntrainer = Seq2SeqTrainer(args=training_args, model=final_model, train_dataset=dataset, data_collator=collator, tokenizer=processor.feature_extractor,\n                        callbacks=[SavePeftModelCallback])\ntrainer.train()\nstate_dict = final_model.state_dict()\ntorch.save(state_dict, '/kaggle/working/model_weights.pth')","metadata":{"execution":{"iopub.status.busy":"2024-07-19T17:28:36.912515Z","iopub.execute_input":"2024-07-19T17:28:36.913221Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"max_steps is given, it will override any value given in num_train_epochs\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='9' max='3000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [   9/3000 01:38 < 11:39:51, 0.07 it/s, Epoch 0.00/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}